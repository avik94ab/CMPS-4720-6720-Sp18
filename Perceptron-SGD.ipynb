{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to 2.7.14 |Anaconda custom (64-bit)| (default, Oct  5 2017, 02:28:52) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "\n",
      "\n",
      "===Importing the necessary libraries for the project===\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created by Avik Bhattacharya\n",
    "CMPS 4720/6720 Homework: Implement your own classification algorithm\n",
    "'''\n",
    "\n",
    "import sys\n",
    "print \"Welcome to %s\\n\\n\"%sys.version\n",
    "print '===Importing the necessary libraries for the project==='\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Sign function is used to activate the neuron\n",
    "'''\n",
    "def activation(x):\n",
    "    if x>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Stochastic gradient descent used for learning weights, online training\n",
    "'''\n",
    "def train_algorithm(data,n_iter,learn_rate):\n",
    "    weights=[]\n",
    "    '''\n",
    "    The weight vector is initialised with all zeros\n",
    "    '''\n",
    "    for i in range(1,np.shape(data)[1]):\n",
    "        weights.append(0)\n",
    "    '''\n",
    "    n_iter is the terminating condition\n",
    "    '''\n",
    "    for i in range (0,n_iter):\n",
    "        '''\n",
    "        k iterates over all the examples one by one for each value of i\n",
    "        Data has been read and manipulated to pad x0 with 1 and also shift the label to the last column.\n",
    "        (Please see the function: read_source)\n",
    "        '''\n",
    "        for k in range(0,len(data)):\n",
    "            inputs=data[k][0:np.shape(data)[1]-1]\n",
    "            label=data[k][np.shape(data)[1]-1:np.shape(data)[1]]\n",
    "            adder=0\n",
    "            for i in range (1,np.shape(data)[1]):\n",
    "                adder += inputs[i-1]*weights[i-1]\n",
    "            '''\n",
    "            Perceptron delta learning rule implemented\n",
    "            '''\n",
    "            guess=activation((adder))\n",
    "            \n",
    "            error=(label-guess)\n",
    "            #print \"Guess=%d ,Target=%d, Error=%d\"%(guess,label ,error)\n",
    "            '''\n",
    "            Update the weight on the basis of error generated in recongnizing the k-th examples \n",
    "            '''\n",
    "            weights=weights+learn_rate*(error*inputs)\n",
    "    return weights   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Learned weights used for sample classification using trained model\n",
    "'''\n",
    "def test_algorithm(data,w):\n",
    "    predicted=[]\n",
    "    weights=w\n",
    "    '''\n",
    "    Take all k examples one by one and predict the labels using the trained neuron\n",
    "    '''\n",
    "    for k in range(0,len(data)):\n",
    "        inputs=data[k][0:np.shape(data)[1]-1]\n",
    "        adder=0\n",
    "        for i in range(1,np.shape(data)[1]):\n",
    "            adder += inputs[i-1]*weights[i-1]\n",
    "        guess=activation(adder)\n",
    "        predicted.append(guess)\n",
    "    '''\n",
    "    return the predicted labels for the k examples to the main function\n",
    "    '''\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Accuracy, TP, TN, FP, FN are evaluated\n",
    "'''\n",
    "def metrics(predicted, labels):\n",
    "    tp=0.0\n",
    "    fp=0.0\n",
    "    tn=0.0\n",
    "    fn=0.0\n",
    "    '''\n",
    "    Checking the predictions and comparing them to the actual labels one by one\n",
    "    '''\n",
    "    for i in range(0,len(labels)):\n",
    "        if (labels[i]==1):\n",
    "            if (predicted[i]==1):\n",
    "                tp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "        if (labels[i]==0):\n",
    "            if (predicted[i]==0):\n",
    "                tn+=1\n",
    "            else:\n",
    "                fp+=1\n",
    "    \n",
    "    accuracy=(tp+tn)*1.0/(tp+tn+fp+fn)\n",
    "    \n",
    "    return accuracy,tp,tn,fp,fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to return a feature set and labels ready to train classifier \n",
    "'''\n",
    "def read_source(filename):\n",
    "    df1=pd.read_csv(filename, header=None)\n",
    "    data=df1.values\n",
    "    '''\n",
    "    for loop used below swaps first and last column because initially\n",
    "    the model was developed considering the last column to be label,\n",
    "    so modified it to keep up with the pattern earlier developed\n",
    "    '''\n",
    "    for i in range (0,len(data)):\n",
    "        temp=data[i][0]\n",
    "        data[i][0]=data[i][np.shape(data)[1]-1]\n",
    "        data[i][np.shape(data)[1]-1]=temp\n",
    "    s=(np.shape(data)[0],np.shape(data)[1]+1)\n",
    "    a=np.ones(s)\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    '''\n",
    "    for i in range(0,len(data)):\n",
    "        if (data[i][np.shape(data)[1]-1]==0):\n",
    "            data[i][np.shape(data)[1]-1]=0\n",
    "    '''\n",
    "    '''\n",
    "    padding extra 1s at x0 positions\n",
    "    '''\n",
    "    for i in range (0,np.shape(data)[1]):\n",
    "        for j in range(0,len(data)):\n",
    "            a[j][i+1]=data[j][i]\n",
    "    data=a\n",
    "    '''\n",
    "    Return totally processed feature set and labels in the sense that the x0 is \n",
    "    padded with 1 and the last column is made the labels.\n",
    "    '''\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef validate_set(arr,percentage):\\n    np.random.shuffle(arr)\\n    if((percentage*1.0/100)*(len(arr))==0):\\n        r=1\\n    else:\\n        r=int(((percentage*1.0/100)*(len(arr))))\\n    m = len(arr)-r\\n    k = np.shape(arr)[1]\\n    s=(m,k)\\n    test=np.zeros(s)\\n    size=(r,np.shape(arr)[1])\\n    val=np.zeros(size)\\n    for i in range(0,r):\\n        val[i][0:np.shape(arr)[1]]=arr[i][0:np.shape(arr)[1]]\\n    for i in range(r,(len(arr))):\\n        test[i-r][0:np.shape(arr)[1]]=arr[i][0:np.shape(arr)[1]]\\n    \\n    return test,val\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def validate_set(arr,percentage):\n",
    "    np.random.shuffle(arr)\n",
    "    if((percentage*1.0/100)*(len(arr))==0):\n",
    "        r=1\n",
    "    else:\n",
    "        r=int(((percentage*1.0/100)*(len(arr))))\n",
    "    m = len(arr)-r\n",
    "    k = np.shape(arr)[1]\n",
    "    s=(m,k)\n",
    "    test=np.zeros(s)\n",
    "    size=(r,np.shape(arr)[1])\n",
    "    val=np.zeros(size)\n",
    "    for i in range(0,r):\n",
    "        val[i][0:np.shape(arr)[1]]=arr[i][0:np.shape(arr)[1]]\n",
    "    for i in range(r,(len(arr))):\n",
    "        test[i-r][0:np.shape(arr)[1]]=arr[i][0:np.shape(arr)[1]]\n",
    "    \n",
    "    return test,val\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef plotter(start,end,step):\\n    final=[]\\n    \\n    y_axis_err=[]\\n    percentage=20\\n    print '\\n Randomly select %d percent of training set for tuning the number of iterations required\\n'%(percentage)\\n    print 'Validating on 10 randomly selected models'\\n    #train_data=read_source('train.txt')\\n    #train_data,val_data=validate_set(train_data,percentage)\\n    learn_rate=0.001\\n    for k in range(0,10):\\n        y_axis=[]\\n        train_data=read_source('train.txt')\\n        train_data,val_data=validate_set(train_data,percentage)\\n        for n_iter in range(start,end,step):\\n            w=train_algorithm(train_data,n_iter,learn_rate)\\n            test_data=val_data\\n            predicted=test_algorithm(test_data,w)\\n            labels=[]\\n            for i in range(0,len(test_data)):\\n                labels.append(test_data[i][np.shape(test_data)[1]-1])\\n                labels[i]=int(labels[i])\\n            accuracy,tp,tn,fp,fn= metrics(predicted,labels)\\n            err=(1-accuracy)\\n            y_axis.append(accuracy)\\n            y_axis_err.append(err)\\n        xaxis = np.arange(start,end,step)\\n        fig, ax = plt.subplots()\\n        ax.plot(xaxis, y_axis, 'b')\\n        ax.set_xlabel('No. of iterations (n_iter)')\\n        ax.set_ylabel('Classification Accuracy')\\n        plt.title('Accuracy w.r.t n_iter')\\n        plt.savefig('n_iter vs acc.pdf')\\n        plt.show()\\n        ymax=max(y_axis)\\n        for i in range(start,end,step):\\n            if(y_axis[i]==ymax and  y_axis[i-1]!=y_axis[i]):\\n                n=i\\n        print 'Best accuracy of %d th model is at n_ter=%d'%(k+1,n)\\n        final.append(n)\\n    return final\\n \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Plotter function\n",
    "'''\n",
    "'''\n",
    "def plotter(start,end,step):\n",
    "    final=[]\n",
    "    \n",
    "    y_axis_err=[]\n",
    "    percentage=20\n",
    "    print '\\n Randomly select %d percent of training set for tuning the number of iterations required\\n'%(percentage)\n",
    "    print 'Validating on 10 randomly selected models'\n",
    "    #train_data=read_source('train.txt')\n",
    "    #train_data,val_data=validate_set(train_data,percentage)\n",
    "    learn_rate=0.001\n",
    "    for k in range(0,10):\n",
    "        y_axis=[]\n",
    "        train_data=read_source('train.txt')\n",
    "        train_data,val_data=validate_set(train_data,percentage)\n",
    "        for n_iter in range(start,end,step):\n",
    "            w=train_algorithm(train_data,n_iter,learn_rate)\n",
    "            test_data=val_data\n",
    "            predicted=test_algorithm(test_data,w)\n",
    "            labels=[]\n",
    "            for i in range(0,len(test_data)):\n",
    "                labels.append(test_data[i][np.shape(test_data)[1]-1])\n",
    "                labels[i]=int(labels[i])\n",
    "            accuracy,tp,tn,fp,fn= metrics(predicted,labels)\n",
    "            err=(1-accuracy)\n",
    "            y_axis.append(accuracy)\n",
    "            y_axis_err.append(err)\n",
    "        xaxis = np.arange(start,end,step)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(xaxis, y_axis, 'b')\n",
    "        ax.set_xlabel('No. of iterations (n_iter)')\n",
    "        ax.set_ylabel('Classification Accuracy')\n",
    "        plt.title('Accuracy w.r.t n_iter')\n",
    "        plt.savefig('n_iter vs acc.pdf')\n",
    "        plt.show()\n",
    "        ymax=max(y_axis)\n",
    "        for i in range(start,end,step):\n",
    "            if(y_axis[i]==ymax and  y_axis[i-1]!=y_axis[i]):\n",
    "                n=i\n",
    "        print 'Best accuracy of %d th model is at n_ter=%d'%(k+1,n)\n",
    "        final.append(n)\n",
    "    return final\n",
    " '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weight vector of length 23 for the trained perceptron is:\n",
      "\n",
      "[ -7.00000000e-01   1.00000000e-01  -1.00000000e-01  -7.00000000e-01\n",
      "  -5.00000000e-01  -2.00000000e-01  -2.00000000e-01   5.00000000e-01\n",
      "   8.00000000e-01   1.00000000e+00  -2.00000000e-01  -1.00000000e-01\n",
      "   1.00000000e-01  -4.00000000e-01   3.00000000e-01   0.00000000e+00\n",
      "  -2.00000000e-01  -1.00000000e-01   5.00000000e-01   1.00000000e+00\n",
      "   1.00000000e-01   3.00000000e-01  -2.77555756e-17]\n",
      "\n",
      "\n",
      "The model built on the basis of 56 iterations performs as below\n",
      "\n",
      "\n",
      "1.Accuracy=46.524064% \n",
      "2.tp=72 \n",
      "3.tn=15 \n",
      "4.fp=0 \n",
      "5.fn=100\n",
      "\n",
      "This is our confusion matrix\n",
      "\n",
      "\tFalse\tTrue\n",
      "False\t15\t0\n",
      "True\t100\t72\n",
      "\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "'''\n",
    "copy pasted the training and testing file into text files and saved them\n",
    "Name of Training set file: train.txt\n",
    "Name of Testing set file: test.txt\n",
    "'''\n",
    "train_data=read_source('train.txt')\n",
    "test_data=read_source('test.txt')\n",
    "n_iter=56 #on the basis of changing values and checking accuracy, didn't do CV\n",
    "learn_rate=0.1 # for AND function change this to 1 to make it converge quicker\n",
    "w=train_algorithm(train_data,n_iter,learn_rate)\n",
    "print \"The weight vector of length %d for the trained perceptron is:\\n\"%(len(w))\n",
    "print w\n",
    "print \"\\n\"\n",
    "predicted=test_algorithm(test_data,w)\n",
    "labels=[]\n",
    "'''\n",
    "Extracting the labels of the test set and storing them in the array 'labels'\n",
    "'''\n",
    "for i in range(0,len(test_data)):\n",
    "    labels.append(test_data[i][np.shape(test_data)[1]-1])\n",
    "    labels[i]=int(labels[i])\n",
    "'''\n",
    "Calling the metrics function to get the performance metrices\n",
    "'''\n",
    "accuracy,tp,tn,fp,fn= metrics(predicted,labels)\n",
    "print \"The model built on the basis of %d iterations performs as below\\n\"%(n_iter)\n",
    "print \"\\n1.Accuracy=%f%% \\n2.tp=%d \\n3.tn=%d \\n4.fp=%d \\n5.fn=%d\\n\"%(accuracy*100,tp,tn,fp,fn)\n",
    "print \"This is our confusion matrix\\n\"\n",
    "print \"\\tFalse\\tTrue\\nFalse\\t%d\\t%d\\nTrue\\t%d\\t%d\\n\"%(tn,fp,fn,tp)\n",
    "print \"Done\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
